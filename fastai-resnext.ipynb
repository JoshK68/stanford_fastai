{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install console_progressbar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tarfile\nimport numpy as np\nimport scipy.io\nimport os\nimport cv2 as cv\nimport shutil\nimport random\nfrom console_progressbar import ProgressBar","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensure_folder(folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n        \ndef save_train_data(fnames, labels, bboxes):\n    src_folder ='../input/stanford-cars-dataset/cars_train/cars_train/'\n    num_samples = len(fnames)\n    train_split = 0.8\n    num_train = int(round(num_samples * train_split))\n    train_indexes = random.sample(range(num_samples), num_train)\n\n    pb = ProgressBar(total=100, prefix='Save train data', suffix='', decimals=3, length=50, fill='=')\n\n    for i in range(num_samples):\n        fname = fnames[i]\n        label = labels[i]\n        (x1, y1, x2, y2) = bboxes[i]\n\n        src_path = os.path.join(src_folder, fname)\n        src_image = cv.imread(src_path)\n        height, width = src_image.shape[:2]\n        # margins of 16 pixels\n        margin = 16\n        x1 = max(0, x1 - margin)\n        y1 = max(0, y1 - margin)\n        x2 = min(x2 + margin, width)\n        y2 = min(y2 + margin, height)\n        # print(\"{} -> {}\".format(fname, label))\n        pb.print_progress_bar((i + 1) * 100 / num_samples)\n\n        if i in train_indexes:\n            dst_folder = '/kaggle/working/data/train/'\n        else:\n            dst_folder = '/kaggle/working/data/valid/'\n\n        dst_path = os.path.join(dst_folder, label)\n        if not os.path.exists(dst_path):\n            os.makedirs(dst_path)\n        dst_path = os.path.join(dst_path, fname)\n\n        crop_image = src_image[y1:y2, x1:x2]\n        dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))\n        cv.imwrite(dst_path, dst_img)\n        \ndef save_test_data(fnames, bboxes):\n    src_folder = '../input/stanford-cars-dataset/cars_test/cars_test/'\n    dst_folder = '/kaggle/working/data/test/'\n    num_samples = len(fnames)\n    pb = ProgressBar(total=100, prefix='Save test data', suffix='', decimals=3, length=50, fill='=')\n\n    for i in range(num_samples):\n        fname = fnames[i]\n        (x1, y1, x2, y2) = bboxes[i]\n        src_path = os.path.join(src_folder, fname)\n        src_image = cv.imread(src_path)\n        height, width = src_image.shape[:2]\n        # margins of 16 pixels\n        margin = 16\n        x1 = max(0, x1 - margin)\n        y1 = max(0, y1 - margin)\n        x2 = min(x2 + margin, width)\n        y2 = min(y2 + margin, height)\n        # print(fname)\n        pb.print_progress_bar((i + 1) * 100 / num_samples)\n\n        dst_path = os.path.join(dst_folder, fname)\n        crop_image = src_image[y1:y2, x1:x2]\n        dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))\n        cv.imwrite(dst_path, dst_img)\n\ndef process_train_data():\n    print(\"Processing train data...\")\n    cars_annos = scipy.io.loadmat('../input/cars-devkit/cars_train_annos.mat')\n    annotations = cars_annos['annotations']\n    annotations = np.transpose(annotations)\n\n    fnames = []\n    class_ids = []\n    bboxes = []\n    labels = []\n\n    for annotation in annotations:\n        bbox_x1 = annotation[0][0][0][0]\n        bbox_y1 = annotation[0][1][0][0]\n        bbox_x2 = annotation[0][2][0][0]\n        bbox_y2 = annotation[0][3][0][0]\n        class_id = annotation[0][4][0][0]\n        labels.append('%04d' % (class_id,))\n        fname = annotation[0][5][0]\n        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n        class_ids.append(class_id)\n        fnames.append(fname)\n\n    labels_count = np.unique(class_ids).shape[0]\n    print(np.unique(class_ids))\n    print('The number of different cars is %d' % labels_count)\n\n    save_train_data(fnames, labels, bboxes)\n\ndef process_test_data():\n    print(\"Processing test data...\")\n    cars_annos = scipy.io.loadmat('../input/cars-devkit/cars_test_annos.mat')\n    annotations = cars_annos['annotations']\n    annotations = np.transpose(annotations)\n\n    fnames = []\n    bboxes = []\n\n    for annotation in annotations:\n        bbox_x1 = annotation[0][0][0][0]\n        bbox_y1 = annotation[0][1][0][0]\n        bbox_x2 = annotation[0][2][0][0]\n        bbox_y2 = annotation[0][3][0][0]\n        fname = annotation[0][4][0]\n        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n        fnames.append(fname)\n\n    save_test_data(fnames, bboxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_width, img_height = 224, 224\n\ncars_meta = scipy.io.loadmat('../input/cars-devkit/cars_meta.mat')\nclass_names = cars_meta['class_names']  # shape=(1, 196)\nclass_names = np.transpose(class_names)\nprint('class_names.shape: ' + str(class_names.shape))\nprint('Sample class_name: [{}]'.format(class_names[8][0][0]))\n\nensure_folder('/kaggle/working/data/train')\nensure_folder('/kaggle/working/data/valid')\nensure_folder('/kaggle/working/data/test')\n\nprocess_train_data()\nprocess_test_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization Data","metadata":{}},{"cell_type":"code","source":"import torchvision\nimport pandas as pd\nimport torch\nimport os\nimport seaborn as sns\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rerorganize Image path","metadata":{}},{"cell_type":"code","source":"# tot_images_viz = no. of images that you want to visualize from each class\n# num_class = no. of classes that you want to visualize\n\ntot_images_viz,num_classes,img_size = 10, 16, 224\n\ntrain_im_dir = './data/train/'\nimg_paths = [train_im_dir + str(i).zfill(4) + '/' + fn for i in range(1,5) \\\n                 for fn in os.listdir(train_im_dir + str(i).zfill(4))[:tot_images_viz] ]    \n    \nimg_dict = {}\nfor i in range(1,num_classes+1):\n    img_dict[i] = list(map(lambda x: train_im_dir + str(i).zfill(4) + '/' + x,\n                      os.listdir(train_im_dir + str(i).zfill(4))[:tot_images_viz]))\n\ndef im2tensor(file_names,bs=num_classes):\n    all_im = torch.zeros((bs,3,img_size,img_size))\n    custom_transform = transforms.Compose([transforms.Resize((img_size, img_size)),                                           \n                                           transforms.ToTensor()])\n    for i,fn in enumerate(file_names):\n        all_im[i,:,:,:] = (custom_transform(Image.open(fn)))\n        \n    return all_im","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Difference Class","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nimgs = []\nclass_name_list = scipy.io.loadmat('../input/stanford-cars-dataset/cars_annos.mat')['class_names'].flatten()\nclass_name_list = list(map(lambda x: x[0],class_name_list))\n\nfor i in range(0,tot_images_viz):\n    fn = [img_dict[j][i] for j in range(1,num_classes+1)]\n    all_im = im2tensor(fn)\n    \n    grid = torchvision.utils.make_grid(all_im,4)\n    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n    im = Image.fromarray(ndarr)\n    im = plt.imshow(im, animated=True) \n    imgs.append([im])\n\nani = animation.ArtistAnimation(fig, imgs, interval=1000, blit=True,\n                                repeat_delay=1000)\nplt.tight_layout()\nplt.title('Visualizing different classes')\n\nani.save('car_images.gif')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Count","metadata":{}},{"cell_type":"code","source":"all_img_dict = {}\nnum_class_visualize,class_count = 196,[]\nfor i in range(1,num_class_visualize+1):\n    class_count.append(len(os.listdir(train_im_dir + str(i).zfill(4))))\n\nclass_count = np.array(class_count)\nclass_count_df = pd.DataFrame(class_count,columns=['class_count'])\nprint(class_count_df.describe())\nprint('Class with min count',class_count.argmin())\nprint('Class with max count',class_count.argmax())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(class_count_df.class_count)\nplt.title('Distibution of class counts')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_dict = {}\nnum_class_visualize = 10\nfor i in range(1,num_class_visualize+1):\n    all_img_dict[i] = list(map(lambda x: train_im_dir + str(i).zfill(4) + '/' + x,\n                      os.listdir(train_im_dir + str(i).zfill(4))))\n\nimages = []\nlabels = []\nresized_img_size = 100\n\nfor class_name,file_names in all_img_dict.items():\n        for fn in file_names:\n            image = np.array(Image.open(fn)).flatten()\n            image = cv.resize(image,(resized_img_size,\n                                      resized_img_size))\n    \n            images.append(image.flatten())\n            labels.append(class_name)\n\n        \nimages = np.array(images)\nprint(images.shape)\nlabels = np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_cols = [ 'pixel'+str(i) for i in range(resized_img_size**2) ]\n\ndf = pd.DataFrame(images,columns=feat_cols)\ndf['y'] = labels\ndf['label'] = df['y'].apply(lambda i: class_name_list[i])\n\nX, y = None, None\n\nprint('Size of the dataframe: {}'.format(df.shape))\n\n# For reproducability of the results\nnp.random.seed(42)\n\nrndperm = np.random.permutation(df.shape[0])\n\npca = PCA(n_components=3)\npca_result = pca.fit_transform(df[feat_cols].values)\n\ndf['pca-one'] = pca_result[:,0]\ndf['pca-two'] = pca_result[:,1] \ndf['pca-three'] = pca_result[:,2]\n\nprint('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization 1","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"pca-one\", y=\"pca-two\",\n    hue=\"y\",\n    palette=sns.color_palette(\"hls\", num_class_visualize),\n    data=df,\n#     legend=\"full\",\n    alpha=1\n)\nplt.legend(class_name_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization 2","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=df.loc[rndperm,:][\"pca-one\"], \n    ys=df.loc[rndperm,:][\"pca-two\"], \n    zs=df.loc[rndperm,:][\"pca-three\"], \n    c=df.loc[rndperm,:][\"y\"], \n    cmap='tab10'\n)\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch Deep Learning Classification","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nfrom fastai import *\nimport numpy as np\nimport pandas as pd\nimport scipy.io as sio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"\ntfms = get_transforms(do_flip=True, max_lighting=0.2, max_zoom=1.03,\n                      max_warp=0.,\n                      xtra_tfms=[rand_crop(), rand_zoom(1, 1.3),\n                                 symmetric_warp(magnitude=(-0.2, 0.2))])\n\n## load Databunch file that ca be directly passed into the pytorch fastai pipeline\ndata = ImageDataBunch.from_folder('data/','train','valid',\n                                  ds_tfms=tfms\n                                  ,size=128,bs=64).normalize(imagenet_stats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.show_batch(rows=3, figsize=(12,9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pretrainedmodels\nimport pretrainedmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=2.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## resnext\n##### Resnext is a next generation network archietecture for image classification. Compare to resnet, it only require minimal extra effort design each path that increase dimension form small to large.","metadata":{}},{"cell_type":"code","source":"def se_resnext50_32x4d(pretrained=False):\n    pretrained = 'imagenet' if pretrained else None\n    model = pretrainedmodels.se_resnext50_32x4d(pretrained=pretrained)\n    return nn.Sequential(*list(model.children()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = cnn_learner(data, se_resnext50_32x4d, pretrained=True, cut=-2,\n                    split_on=lambda m: (m[0][3], m[1]), \n                    metrics=[accuracy])\nlearn.loss_fn = FocalLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(32, max_lr=slice(2e-2), wd=1e-5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('SE_ResNext50_1');\nlearn.unfreeze();\nlearn = learn.clip_grad();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.load('SE_ResNext50_1');\nlearn.unfreeze();\nlearn = learn.clip_grad();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = [3e-3/100, 3e-3/20, 3e-3/10]\nlearn.fit_one_cycle(36, lr, wd=1e-7)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.export('/kaggle/working/fastai_resnet.pkl');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting on the test set","metadata":{}},{"cell_type":"code","source":"labels = sio.loadmat('../input/cars-devkit/cars_test_annos_withlabels.mat')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = []\nfor i in range(8041):\n    x.append(np.transpose(np.array(labels['annotations']['fname']))[i][0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame(data=np.transpose(np.array(labels['annotations']['class'],dtype=np.int)),\n                  index=x)\n\ndf.to_csv('/kaggle/working/data/test_labels.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = load_learner('/kaggle/working/','fastai_resnet.pkl', test= \n                     ImageList.from_csv('/kaggle/working/data','test_labels.csv',folder='test'))\npreds,y = learn.TTA(ds_type=DatasetType.Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(preds.cpu().numpy()).to_csv('raw_test_preds.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=preds;a.shape\nb=np.array(labels['annotations']['class'],dtype=np.int)-1;b.shape \nb = torch.from_numpy(b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc=accuracy(a,b);acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_preds = torch.argmax(preds,1).cpu().numpy()\nout = open('result.txt', 'a')\nfor val in labelled_preds:\n    out.write('{}\\n'.format(str(val+1)))\nout.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf data/","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}